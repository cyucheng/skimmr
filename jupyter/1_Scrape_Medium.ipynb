{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape articles from Medium.com\n",
    "\n",
    "### Get corpus of articles for the project by scraping popular articles from Medium.com.\n",
    "\n",
    "I originally planned to scrape all articles from a particular topic/tag on Medium.com (e.g. technology or politics), but I found that many articles did not have many likes/recommends and lacked 'top highlights', presumably because not enough people highlighted sentences to allow for a consensus.\n",
    "\n",
    "Instead, I scraped the 30 most popular articles per day from Medium.com to ensure that most articles would include a 'top highlight', then removed duplicates from the corpus. Originally I spaced the dates scraped 10 days apart; then I ran the script again using dates offset by 5 days. Thus, the final dataset includes unique articles scraped from Medium.com's daily list of the most popular, with a spacing of 5 days.\n",
    "\n",
    "I used Selenium Webdriver and BeautifulSoup to scrape the html and identify the full text and highlights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170606_10-45-58\n",
      "range(0, 812, 10)\n",
      "may-31-2017\n",
      "may-31-2017 Done!\n",
      "may-21-2017\n",
      "may-21-2017 Done!\n",
      "may-11-2017\n",
      "may-11-2017 Done!\n",
      "may-01-2017\n",
      "may-01-2017 Done!\n",
      "april-21-2017\n",
      "april-21-2017 Done!\n",
      "april-11-2017\n",
      "april-11-2017 Done!\n",
      "april-01-2017\n",
      "april-01-2017 Done!\n",
      "march-22-2017\n",
      "march-22-2017 Done!\n",
      "march-12-2017\n",
      "march-12-2017 Done!\n",
      "march-02-2017\n",
      "march-02-2017 Done!\n",
      "february-20-2017\n",
      "february-20-2017 Done!\n",
      "february-10-2017\n",
      "february-10-2017 Done!\n",
      "january-31-2017\n",
      "january-31-2017 Done!\n",
      "january-21-2017\n",
      "january-21-2017 Done!\n",
      "january-11-2017\n",
      "january-11-2017 Done!\n",
      "january-01-2017\n",
      "january-01-2017 Done!\n",
      "december-22-2016\n",
      "december-22-2016 Done!\n",
      "december-12-2016\n",
      "december-12-2016 Done!\n",
      "december-02-2016\n",
      "december-02-2016 Done!\n",
      "november-22-2016\n",
      "november-22-2016 Done!\n",
      "november-12-2016\n",
      "november-12-2016 Done!\n",
      "november-02-2016\n",
      "november-02-2016 Done!\n",
      "october-23-2016\n",
      "october-23-2016 Done!\n",
      "october-13-2016\n",
      "october-13-2016 Done!\n",
      "october-03-2016\n",
      "october-03-2016 Done!\n",
      "september-23-2016\n",
      "september-23-2016 Done!\n",
      "september-13-2016\n",
      "september-13-2016 Done!\n",
      "september-03-2016\n",
      "september-03-2016 Done!\n",
      "august-24-2016\n",
      "august-24-2016 Done!\n",
      "august-14-2016\n",
      "august-14-2016 Done!\n",
      "august-04-2016\n",
      "august-04-2016 Done!\n",
      "july-25-2016\n",
      "july-25-2016 Done!\n",
      "july-15-2016\n",
      "july-15-2016 Done!\n",
      "july-05-2016\n",
      "july-05-2016 Done!\n",
      "june-25-2016\n",
      "june-25-2016 Done!\n",
      "june-15-2016\n",
      "june-15-2016 Done!\n",
      "june-05-2016\n",
      "june-05-2016 Done!\n",
      "may-26-2016\n",
      "may-26-2016 Done!\n",
      "may-16-2016\n",
      "may-16-2016 Done!\n",
      "may-06-2016\n",
      "may-06-2016 Done!\n",
      "april-26-2016\n",
      "april-26-2016 Done!\n",
      "april-16-2016\n",
      "april-16-2016 Done!\n",
      "april-06-2016\n",
      "april-06-2016 Done!\n",
      "march-27-2016\n",
      "march-27-2016 Done!\n",
      "march-17-2016\n",
      "march-17-2016 Done!\n",
      "march-07-2016\n",
      "march-07-2016 Done!\n",
      "february-26-2016\n",
      "february-26-2016 Done!\n",
      "february-16-2016\n",
      "february-16-2016 Done!\n",
      "february-06-2016\n",
      "february-06-2016 Done!\n",
      "january-27-2016\n",
      "january-27-2016 Done!\n",
      "january-17-2016\n",
      "january-17-2016 Done!\n",
      "january-07-2016\n",
      "january-07-2016 Done!\n",
      "december-28-2015\n",
      "december-28-2015 Done!\n",
      "december-18-2015\n",
      "december-18-2015 Done!\n",
      "december-08-2015\n",
      "december-08-2015 Done!\n",
      "november-28-2015\n",
      "november-28-2015 Done!\n",
      "november-18-2015\n",
      "november-18-2015 Done!\n",
      "november-08-2015\n",
      "november-08-2015 Done!\n",
      "october-29-2015\n",
      "october-29-2015 Done!\n",
      "october-19-2015\n",
      "october-19-2015 Done!\n",
      "october-09-2015\n",
      "october-09-2015 Done!\n",
      "september-29-2015\n",
      "september-29-2015 Done!\n",
      "september-19-2015\n",
      "september-19-2015 Done!\n",
      "september-09-2015\n",
      "september-09-2015 Done!\n",
      "august-30-2015\n",
      "august-30-2015 Done!\n",
      "august-20-2015\n",
      "august-20-2015 Done!\n",
      "august-10-2015\n",
      "august-10-2015 Done!\n",
      "july-31-2015\n",
      "july-31-2015 Done!\n",
      "july-21-2015\n",
      "july-21-2015 Done!\n",
      "july-11-2015\n",
      "july-11-2015 Done!\n",
      "july-01-2015\n",
      "july-01-2015 Done!\n",
      "june-21-2015\n",
      "june-21-2015 Done!\n",
      "june-11-2015\n",
      "june-11-2015 Done!\n",
      "june-01-2015\n",
      "june-01-2015 Done!\n",
      "may-22-2015\n",
      "may-22-2015 Done!\n",
      "may-12-2015\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import urllib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import re\n",
    "import os, time\n",
    "import pandas as pd\n",
    "from datetime import timedelta, date\n",
    "import html5lib\n",
    "\n",
    "\n",
    "#### Estimate number of articles will get\n",
    "# 818 days between 3/11/2015 and 6/5/2017 -- 818/10 (every 10 days) * 30 (articles per 10-days)  =2454 articles\n",
    "# 818 / 10 (every 10 days) * 6 (min per 30-article day) / 60 (min/hour) = 8.18 hr\n",
    "\n",
    "#### URL formats for automated group scraping\n",
    "## format for scraping by a topic/tag:\n",
    "# https://medium.com/tag/politics/archive/2015/03/11\n",
    "# https://medium.com/tag/technology/archive/2015/03/11\n",
    "## format for scraping popular articles:\n",
    "# https://medium.com/browse/top/march-11-2016\n",
    "\n",
    "#### URLs for specific test cases\n",
    "# webpage = 'https://medium.com/microsoft-design/if-you-want-to-be-creative-dont-be-data-driven-55db74078eda'\n",
    "# webpage = 'https://medium.com/startup-grind/fueling-the-ai-gold-rush-7ae438505bc2'\n",
    "# webpage = 'https://betterhumans.coach.me/the-day-reading-died-c8fd8da7814'\n",
    "# webpage = 'https://electricliterature.com/men-recommend-david-foster-wallace-to-me-7889a9dc6f03'\n",
    "# webpage = 'https://medium.com/dualcores-studio/make-an-android-custom-view-publish-and-open-source-99a3d86df228'\n",
    "# webpage = 'https://medium.com/@emmalindsay/whose-pain-counts-6e6b3dd287f5'\n",
    "# webpage = 'https://backchannel.com/how-the-trendiest-grilled-cheese-venture-got-burnt-aa627b0c7ae1'\n",
    "# webpage = 'https://medium.com/@bindingwave/florida-man-goes-undercover-at-a-trump-rally-51ec77e08eed'\n",
    "\n",
    "\n",
    "# Set a date range to scrape\n",
    "def daterange(start_date, end_date):\n",
    "    print( range(0, int ((end_date - start_date).days), 10) )\n",
    "    for n in range(0, int ((end_date - start_date).days), 10):\n",
    "        yield end_date - timedelta(n)\n",
    "\n",
    "class HighlightScraper(object):\n",
    "\n",
    "    def __init__(self, dates, outhigh, outurls, outfull, outhtml):\n",
    "        self.driver = webdriver.PhantomJS()\n",
    "#         self.driver.implicitly_wait(60)    # testing implicitly_wait to allow html javascript to load\n",
    "#         self.link = link\n",
    "#         print( link )\n",
    "\n",
    "    def scrape_highlight(self, dates, outhigh, outurls, outfull, outhtml):\n",
    "\n",
    "        start_date = dates[0]\n",
    "        end_date = dates[1]\n",
    "\n",
    "        idurl = 1\n",
    "        idscraped = 1\n",
    "\n",
    "        # Get list of links for single date\n",
    "        for single_date in daterange(start_date, end_date):\n",
    "            date_str = single_date.strftime('%B-%d-%Y').lower()   # e.g. march-11-2016\n",
    "            print( date_str )\n",
    "            link = 'https://medium.com/browse/top/'+date_str+'?limit=30'\n",
    "            self.driver.get(link)     # url = self.driver.current_url\n",
    "\n",
    "            html1 = self.driver.page_source\n",
    "            # \"Read more...\" button links to actual articles\n",
    "            readmore = SoupStrainer('a',{'class': 'button button--smaller button--chromeless u-baseColor--buttonNormal'})\n",
    "            urls = []\n",
    "            for sub in BeautifulSoup(html1, 'lxml', parse_only=readmore):\n",
    "                if sub == 'html':\n",
    "                    continue\n",
    "                elif sub != 'html':\n",
    "                    if sub.has_attr('href'):\n",
    "                        urls.append( sub['href'] )\n",
    "                        outurls.write(date_str+'\\t'+str(idurl)+'\\t'+str(sub['href'])+'\\n')\n",
    "                        idurl += 1\n",
    "                \n",
    "            # Get highlight for each link (article) per single date\n",
    "            for url in urls:\n",
    "                self.driver.get(url)\n",
    "                time.sleep(10)\n",
    "                html2 = self.driver.execute_script('return document.documentElement.innerHTML;')\n",
    "#                 print( html2 )\n",
    "#                 highlight = SoupStrainer('span',{'class': 'markup--quote'}) \n",
    "                soup = BeautifulSoup(html2, 'lxml')\n",
    "                outhtml.write(date_str+'\\t'+str(idscraped)+'\\t'+str(soup)+'\\n\\n')\n",
    "#                 time.sleep(5)\n",
    "                txt1 = soup.find_all(class_='markup--quote', attrs={'data-creator-ids':'anon'}) # 'markup--quote' = HTML label for highlights\n",
    "#                 if txt1:\n",
    "#                     txt2 = txt1[0].encode('ascii','ignore')\n",
    "#                     print('ascii')\n",
    "#                 else:\n",
    "                txt2 = re.sub('<[^>]+>', '', str(txt1) )[1:-1]\n",
    "                outhigh.write(date_str+'\\t'+str(idscraped)+'\\t'+txt2+'\\n')\n",
    "            \n",
    "                full = BeautifulSoup(html2, 'lxml') #, parse_only=SoupStrainer('p'))\n",
    "                full1 = re.sub('<[^>]+>', '', str(full) )\n",
    "                outfull.write(date_str+'\\t'+str(idscraped)+'\\t'+full1+'\\n')\n",
    "                idscraped += 1\n",
    "            print( date_str+' Done!' )\n",
    "\n",
    "\n",
    "    def scrape(self, dates, outhigh, outurls, outfull, outhtml):\n",
    "        self.scrape_highlight(dates, outhigh, outurls, outfull, outhtml)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    currdir = os.getcwd()\n",
    "    timest = time.strftime('%Y%m%d_%H-%M-%S')\n",
    "    outhigh = open('/Users/clarencecheng/Dropbox/~Insight/skimr/datasets/highlights_'+timest+'.txt','w')\n",
    "    outurls = open('/Users/clarencecheng/Dropbox/~Insight/skimr/datasets/urls_'+timest+'.txt','w')\n",
    "    outfull = open('/Users/clarencecheng/Dropbox/~Insight/skimr/datasets/fulltext_'+timest+'.txt','w')\n",
    "    outhtml = open('/Users/clarencecheng/Dropbox/~Insight/skimr/datasets/fullhtml_'+timest+'.txt','w')\n",
    "    \n",
    "#    dates = [date(2015,3,11), date(2017,6,5)]\n",
    "    dates = [date(2015,3,11), date(2017,5,31)]\n",
    "\n",
    "    print( timest )\n",
    "    scraper = HighlightScraper(dates, outhigh, outurls, outfull, outhtml)\n",
    "    scraper.scrape(dates, outhigh, outurls, outfull, outhtml)\n",
    "    \n",
    "    outhigh.close()\n",
    "    outurls.close()\n",
    "    outfull.close()\n",
    "    outhtml.close()\n",
    "    \n",
    "    timest = time.strftime('%Y%m%d_%H-%M-%S')\n",
    "    print( timest )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## isolate fullhtml lines from fullhtml file\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "fhtmlfile = open('/Users/clarencecheng/Dropbox/~Insight/skimr/datasets/fullhtml_20170606_10-45-58_edit_isolate.txt','w')\n",
    "with open('/Users/clarencecheng/Dropbox/~Insight/skimr/datasets/fullhtml_20170606_10-45-58_edit.txt','r') as fhtml:\n",
    "    fullh = [line for line in fhtml if '\\t' in line]      # Isolate fullhtml lines from fullhtml file\n",
    "    \n",
    "fhtmlfile.write(''.join(fullh))\n",
    "\n",
    "fhtmlfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
